import modal

# å®šä¹‰è¦ä¸‹è½½çš„æ¨¡å‹ID
MODEL_ID = "Tongyi-MAI/Z-Image-Turbo"

# ==============================================================================
# 1. å®šä¹‰ GPU é•œåƒ
# ==============================================================================
def download_model_to_image():
    from huggingface_hub import snapshot_download
    print(f"æ­£åœ¨æ„å»ºé•œåƒæ—¶ä¸‹è½½æ¨¡å‹: {MODEL_ID} ...")
    snapshot_download(repo_id=MODEL_ID, ignore_patterns=["*.msgpack", "*.bin", "*.h5"])

image_gpu = (
    modal.Image.debian_slim(python_version="3.10")
    .run_commands("python -m pip install --upgrade pip")
    .apt_install("git")
    .pip_install(
        "torch",
        "torchvision",
        extra_index_url="https://download.pytorch.org/whl/cu121"
    )
    .pip_install(
        "transformers", 
        "accelerate", 
        "sentencepiece", 
        "huggingface_hub", 
        "protobuf"
    )
    .pip_install("git+https://github.com/huggingface/diffusers.git")
    .run_function(download_model_to_image)
    .env({"PYTORCH_ALLOC_CONF": "expandable_segments:True"})
)

app = modal.App("z-image-turbo-api", image=image_gpu)

# ==============================================================================
# 2. æ ¸å¿ƒæ¨ç†é€»è¾‘ (é€šç”¨å¼•æ“)
#    è¿™ä¸ªç±»ä¸ç»‘å®šå…·ä½“æ˜¾å¡ï¼Œåªè´Ÿè´£åŠ è½½æ¨¡å‹å’Œç”»å›¾ï¼Œä¾›åé¢ä¸¤ä¸ªç±»è°ƒç”¨
# ==============================================================================
class InferenceEngine:
    def load_model(self):
        import torch
        from diffusers import ZImagePipeline
        
        print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹æ ¸å¿ƒ...")
        self.pipe = ZImagePipeline.from_pretrained(
            MODEL_ID,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            local_files_only=True,
        )
        self.pipe.enable_sequential_cpu_offload()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")

    def run(self, prompt, width, height, steps):
        import io
        import torch
        
        torch.cuda.empty_cache()
        with torch.inference_mode():
            image = self.pipe(
                prompt=prompt,
                num_inference_steps=steps, 
                guidance_scale=0.0,
                width=width,
                height=height
            ).images[0]

        byte_stream = io.BytesIO()
        image.save(byte_stream, format="JPEG")
        return byte_stream.getvalue()

# ==============================================================================
# 3. å®šä¹‰ä¸¤ä¸ªä¸åŒçš„ GPU åç«¯
# ==============================================================================

# åç«¯ A: é€šç”¨èŠ‚ç‚¹ (NVIDIA A10G)
# é€‚åˆè·‘ 1024x1024 åŠä»¥ä¸‹çš„ä»»åŠ¡ã€‚
@app.cls(
    image=image_gpu,
    gpu="A10G",  # <--- è¿™é‡ŒæŒ‡å®š A10G
    scaledown_window=300,
    timeout=600,
)
class ModelA10G:
    @modal.enter()
    def setup(self):
        self.engine = InferenceEngine()
        self.engine.load_model()

    @modal.method()
    def generate(self, prompt: str, width: int, height: int, steps: int):
        print(f"âš¡ [A10G é€šç”¨èŠ‚ç‚¹] å¤„ç†ä»»åŠ¡: {width}x{height}")
        return self.engine.run(prompt, width, height, steps)

# åç«¯ B: æ——èˆ°èŠ‚ç‚¹ (NVIDIA A100)
# é€‚åˆè·‘ 2k è¶…å¤§åˆ†è¾¨ç‡çš„ä»»åŠ¡ã€‚
@app.cls(
    image=image_gpu,
    gpu="A100", # <--- è¿™é‡ŒæŒ‡å®š A100
    scaledown_window=300,
    timeout=600,
)
class ModelA100:
    @modal.enter()
    def setup(self):
        self.engine = InferenceEngine()
        self.engine.load_model()

    @modal.method()
    def generate(self, prompt: str, width: int, height: int, steps: int):
        print(f"ğŸš€ [A100 æ——èˆ°èŠ‚ç‚¹] å¤„ç†ä»»åŠ¡: {width}x{height}")
        return self.engine.run(prompt, width, height, steps)

# ==============================================================================
# 4. API æ¥å£ (æ™ºèƒ½è°ƒåº¦å™¨)
# ==============================================================================
@app.function(
    image=modal.Image.debian_slim().pip_install("fastapi[standard]"), 
    scaledown_window=300
)
@modal.fastapi_endpoint(docs=True)
def generate(
    prompt: str = "A cinematic shot of a futuristic city",
    width: int = 1024,
    height: int = 1024,
    steps: int = 4
):
    from fastapi import Response
    
    # å‚æ•°ä¿®æ­£
    if width > 2048: width = 2048
    if height > 2048: height = 2048
    if width < 256: width = 256
    if height < 256: height = 256
    width = (width // 8) * 8
    height = (height // 8) * 8
    if steps > 20: steps = 20
    if steps < 1: steps = 1
    
    # ğŸ§  ã€æ™ºèƒ½è°ƒåº¦é€»è¾‘ã€‘
    total_pixels = width * height
    threshold = 1024 * 1024 # 100ä¸‡åƒç´  (å³æ ‡å‡† 1024x1024)
    
    try:
        if total_pixels <= threshold:
            # å¦‚æœå›¾ç‰‡æ¯”è¾ƒå°ï¼Œæ´¾ç»™ A10G
            print(f"ğŸš¦ è°ƒåº¦: {width}x{height} -> A10G (é€šç”¨)")
            jpg_bytes = ModelA10G().generate.remote(prompt, width, height, steps)
        else:
            # å¦‚æœå›¾ç‰‡å¾ˆå¤§ (2k)ï¼Œæ´¾ç»™ A100
            print(f"ğŸš¦ è°ƒåº¦: {width}x{height} -> A100 (æ——èˆ°)")
            jpg_bytes = ModelA100().generate.remote(prompt, width, height, steps)
            
        return Response(
            content=jpg_bytes, 
            media_type="image/jpeg",
            headers={"Access-Control-Allow-Origin": "*"} 
        )
    except Exception as e:
        return Response(content=f"Error: {str(e)}", status_code=500)